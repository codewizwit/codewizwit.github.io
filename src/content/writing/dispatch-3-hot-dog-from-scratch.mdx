---
title: "Would You Eat a Hot Dog You Prompted AI to Make from Scratch?"
description: "Depends if I've made and eaten enough hot dogs to catch when something's off. AI is an amplifier. If you've got taste and fundamentals, it supercharges you."
type: dispatch
number: 3
publishDate: 2026-01-17
tags: ["responsible-ai", "education", "critical-thinking", "mentorship"]
draft: false
canonicalUrl: "https://codewizwit.medium.com/dispatch-3-would-you-eat-a-hot-dog-you-prompted-ai-to-make-from-scratch-78736038c645"
---

Depends if I've made and eaten enough hot dogs to catch when something's off. I would also need to question: Does it have access to the right ingredients? Would I know what to check for at each step of the process?

AI is an amplifier. If you've got taste and fundamentals, it supercharges you. If you don't, you're just plating whatever comes out and hoping for the best.

This works fine if you already know your stuff. But what happens to the people still learning how to cook?

## The Quiet Experiment

Junior devs are shipping code faster than ever, which is great. But are they really learning?

I've watched engineers prompt their way to working code they can't explain. Tests pass, logic holds, builds pass. But they may not be able to really understand the code, or fix larger systematic issues that can arise when LLMs are not aware of context outside of the codebase.

Here's the thing: they did exactly what they were told. Use the tools, be productive.

We just need to think about whether productivity and learning are the same thing. (Spoiler: they're not.)

## Bigger Than Code

I recently met a humanities professor and we ended up circling the same problem from completely different directions.

His students are using AI to write papers and there is no sign that anyone actually wrestled with the ideas.

He can't just ban AI. That doesn't prepare anyone for reality. But if he lets them use it freely, they skip the part where they learn to think. Sound familiar?

The engineer who can't debug their own code. The student who can't form their own argument. The new hire who can't write an email without prompting ChatGPT first. It's all the same problem.

## What's Actually At Stake

Critical thinking comes from getting stuck and finding your way out. Voice comes from writing badly until you don't. Judgment comes from making mistakes and having to live with them.

These are the foundation of independent thought.

If we shortcut all of that, we're facing a new generation that won't know how to think without a prompt. People who can't form an opinion until they've asked a model first. Who can't sit with ambiguity, can't trust their own judgment, can't function when the tool isn't available or doesn't have an answer.

That's a chilling dependency. And it doesn't stay contained to the workplace. It bleeds into how people make decisions, form beliefs, engage with the world.

What happens to innovation when nobody's comfortable with not knowing? Or to democracy when people can't construct an argument on their own? To art, science, any field that requires sitting in discomfort long enough to find something new?

The tools will glitch. The situations will surprise you. And that IS life. It's our responsibility to make sure we train the next generation to be able to accelerate learning while using AI tools without creating a full dependency on them.

## What We Could Actually Do

Banning AI at this point is unrealistic and impossible. The answer is reshaping how we teach and mentor around it.

Learning with AI has to be about process. And that starts with two things: accountability and skill.

**The accountability piece:**

Everyone needs to be able to defend their work. Can't explain what the AI wrote? Don't ship it. Can't walk through your reasoning? Don't submit it. Can't debug it when it breaks? You're not done yet. Code review becomes a teaching opportunity through human conversation. Feedback becomes dialogue.

**The skill piece:**

Teach people how to use these tools to learn. Coach them to ask "why" when AI gives an answer. Challenge the output instead of accepting it. Prompt for sources and reasoning. Treat AI like a sparring partner.

**The real unlock:**

Learning to control context. Understanding what information the model needs, what assumptions it's making, how to steer it toward better answers. That's a skill. It takes practice. And it's built on top of actually understanding the domain.

Pair juniors with seniors on the weird bugs. Have students workshop their thinking before they ever touch a draft. Build in friction on purpose for practice.

The tools aren't going anywhere. We get to decide whether we're training people to use AI tools while building the skills they need to learn without complete dependence.

## Who Could Get Left Behind

The people most at risk are the ones who never got a chance to learn without it.

Lowering the barrier to entry is genuinely good. But entry and mastery aren't the same thing. We can let everyone into the kitchen and still make sure they learn to cook.

Know your ingredients. Taste as you go. And don't serve what you can't confidently stand behind.

No Human Left Behind.
